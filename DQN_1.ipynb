{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "from gym.wrappers import RecordVideo, RecordEpisodeStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(episode=0):\n",
    "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
    "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN (nn.Module):\n",
    "    def __init__(self, n_actions, hidden_size, obs_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size), # Camada linear com 4 entradas e 8 saídas\n",
    "            nn.ReLU(), # Função de ativação ReLU\n",
    "            nn.Linear(hidden_size, hidden_size),# Camada linear com 4 entradas e 8 saídas\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Policy\n",
    "#  técnica amplamente usada em Reinforcement Learning (RL) para balancear exploração e exploração durante o treinamento de agentes.\n",
    "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()  # Escolhe uma ação aleatória\n",
    "    else:\n",
    "        state = torch.tensor([state]).to(device)  # Converte o estado para tensor e move para o dispositivo (CPU/GPU)\n",
    "        q_values = net(state)  # Calcula os valores Q(s, a) usando a rede neural\n",
    "        _, action = torch.max(q_values, dim=1)  # Seleciona a ação com o maior valor Q\n",
    "        action = int(action.item())  # Converte a ação para um inteiro\n",
    "    return action  # Retorna a ação escolhida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Buffer replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Buffer replay\n",
    "#O principal motivo do uso do Replay Buffer em algoritmos de Reinforcement Learning (RL), especialmente no Deep Q-Learning (DQN), \n",
    "#é quebrar a correlação entre as amostras de treinamento, \n",
    "#o que leva a um aprendizado mais estável e eficiente. \n",
    "#Ele também permite o reaproveitamento de experiências, reduzindo a necessidade de interagir continuamente com o ambiente.\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    def __init__(self, buffer, sample_size = 200): \n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Create Enviroment\n",
    "\n",
    "# def create_enviroment(name):\n",
    "#     env = gym.make(name,  render_mode=\"rgb_array\")\n",
    "#     env = RecordVideo(env, video_folder='./videos')\n",
    "#     return env\n",
    "\n",
    "\n",
    "# env = create_enviroment(\"LunarLander-v2\")\n",
    "\n",
    "# # Reset the environment to start\n",
    "# state = env.reset()\n",
    "# # Run for 1000 timesteps\n",
    "# for _ in range(1000):\n",
    "#       # Render the environment\n",
    "#     action = env.action_space.sample()  # Take a random action\n",
    "#     # print(\"Action taken:\", action)\n",
    "\n",
    "#     # Do this action in the environment and get\n",
    "#     # next_state, reward, done and info\n",
    "#     _, observation, reward, done, info = env.step(action)\n",
    "#     # print('Observation Space: ', observation)\n",
    "#     # print('Reward: ', reward)\n",
    "\n",
    "#     # If the episode is done (CartPole has fallen), reset the environment\n",
    "#     if done:\n",
    "#         state = env.reset()\n",
    "\n",
    "# env.close()  # Close the rendering window\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igor8\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\igor8\\Desktop\\project_reinforcemen_learning\\RL-LunarLander-v2\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\igor8\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "MoviePy - Building video c:\\Users\\igor8\\Desktop\\project_reinforcemen_learning\\RL-LunarLander-v2\\videos\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video c:\\Users\\igor8\\Desktop\\project_reinforcemen_learning\\RL-LunarLander-v2\\videos\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready c:\\Users\\igor8\\Desktop\\project_reinforcemen_learning\\RL-LunarLander-v2\\videos\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "def create_enviroment(name):\n",
    "    env = gym.make(name,  render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x : x % 50 = 0)\n",
    "    env - RecordEpisodeStatistics(env)\n",
    "    return env\n",
    "\n",
    "env = create_enviroment(\"LunarLander-v2\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for _ in range(10000):\n",
    "\n",
    "   action = env.action_space.sample()\n",
    "\n",
    "   print(action)\n",
    "\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      break\n",
    "      \n",
    "\n",
    "\n",
    "#observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning(LightningModule):\n",
    "\n",
    "    # Initialize\n",
    "    def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000, batch_size=256,\n",
    "                lr=le-3, hidden_size=128, gamma=0.99, loss_fn=F.smooth_l1_loss, optim=AdamW,\n",
    "                eps_start=1.0, eps_end=0.15, eps_last_episode=100, samples_per_epoch=10_000, sync_rate=10\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.env = create_enviroment(env_name)\n",
    "\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "\n",
    "        n_actions = self.env.action_space.n\n",
    "\n",
    "        self.q_net = DQN (hidden_size, obs_size, n_actions)\n",
    "\n",
    "        self.target_q_net = copy.deepcopy(self.q_net)\n",
    "\n",
    "        self.policy = policy\n",
    "        self.buffer  = ReplayBuffer(capacity=capacity)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
    "            print(f\"{len(self.buffer)} Sample in experience buffer. Filling...\")\n",
    "            self.play_episode(epsilon=self.hparams.eps_start)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def play_episode (self, epsilon = 0.):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "\n",
    "            \n",
    "            while not done:\n",
    "\n",
    "                if  policy:\n",
    "                    action = self.policy(state, self.env, self.q_net, epsilon=epsilon)\n",
    "                else:\n",
    "                    action = self.env.action_space.sample()\n",
    "\n",
    "                action = policy(state, self.env, self.q_net, epsilon=epsilon)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                exp = (state, action, reward, done, next_state )\n",
    "                self.buffer.append(exp)\n",
    "                state = next_state\n",
    "\n",
    "\n",
    "    # Forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.q_net(x)\n",
    "\n",
    "    # Configure optimizers\n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
    "        return [q_net_optimizer]\n",
    "\n",
    "    # Create dataloader\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.buffer,self.hparams.samples_per_epoch)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    # Training step\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        action = actions.unsqueeze()  \n",
    "        rewards = rewards.unsqueeze()\n",
    "        dones = dones.unsqueeze()\n",
    "\n",
    "    # Training epoch end\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
