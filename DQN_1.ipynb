{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from gym.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(episode=0):\n",
    "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
    "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN (nn.Module):\n",
    "    def __init__(self, n_actions, hidden_size, obs_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size), # Camada linear com 4 entradas e 8 saídas\n",
    "            nn.ReLU(), # Função de ativação ReLU\n",
    "            nn.Linear(hidden_size, hidden_size),# Camada linear com 4 entradas e 8 saídas\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Policy\n",
    "#  técnica amplamente usada em Reinforcement Learning (RL) para balancear exploração e exploração durante o treinamento de agentes.\n",
    "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()  # Escolhe uma ação aleatória\n",
    "    else:\n",
    "        state = torch.tensor([state]).to(device)  # Converte o estado para tensor e move para o dispositivo (CPU/GPU)\n",
    "        q_values = net(state)  # Calcula os valores Q(s, a) usando a rede neural\n",
    "        _, action = torch.max(q_values, dim=1)  # Seleciona a ação com o maior valor Q\n",
    "        action = int(action.item())  # Converte a ação para um inteiro\n",
    "    return action  # Retorna a ação escolhida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Buffer replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Buffer replay\n",
    "#O principal motivo do uso do Replay Buffer em algoritmos de Reinforcement Learning (RL), especialmente no Deep Q-Learning (DQN), \n",
    "#é quebrar a correlação entre as amostras de treinamento, \n",
    "#o que leva a um aprendizado mais estável e eficiente. \n",
    "#Ele também permite o reaproveitamento de experiências, reduzindo a necessidade de interagir continuamente com o ambiente.\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    def __init__(self, buffer, sample_size = 200): \n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Create Enviroment\n",
    "\n",
    "# def create_enviroment(name):\n",
    "#     env = gym.make(name,  render_mode=\"rgb_array\")\n",
    "#     env = RecordVideo(env, video_folder='./videos')\n",
    "#     return env\n",
    "\n",
    "\n",
    "# env = create_enviroment(\"LunarLander-v2\")\n",
    "\n",
    "# # Reset the environment to start\n",
    "# state = env.reset()\n",
    "# # Run for 1000 timesteps\n",
    "# for _ in range(1000):\n",
    "#       # Render the environment\n",
    "#     action = env.action_space.sample()  # Take a random action\n",
    "#     # print(\"Action taken:\", action)\n",
    "\n",
    "#     # Do this action in the environment and get\n",
    "#     # next_state, reward, done and info\n",
    "#     _, observation, reward, done, info = env.step(action)\n",
    "#     # print('Observation Space: ', observation)\n",
    "#     # print('Reward: ', reward)\n",
    "\n",
    "#     # If the episode is done (CartPole has fallen), reset the environment\n",
    "#     if done:\n",
    "#         state = env.reset()\n",
    "\n",
    "# env.close()  # Close the rendering window\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def create_enviroment(name):\n",
    "    env = gym.make(name,  render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x : x % 50 == 0)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    env = TimeLimit(env, max_episode_steps=400)\n",
    "    return env\n",
    "\n",
    "env = create_enviroment(\"LunarLander-v2\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for _ in range(10000):\n",
    "\n",
    "   action = env.action_space.sample()\n",
    "\n",
    "   print(action)\n",
    "\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      break\n",
    "      \n",
    "\n",
    "env.reset()\n",
    "#observation, info = env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning(LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                    env_name,                # Nome do ambiente (ex.: \"LunarLander-v2\"). Define o ambiente com o qual o agente interage.\n",
    "                    policy=epsilon_greedy,   # Política usada para seleção de ações. O padrão é `epsilon-greedy`.\n",
    "                    capacity=10_000,         # Capacidade máxima do Replay Buffer. Limita o número de transições armazenadas.\n",
    "                    batch_size=256,          # Tamanho do batch para treinamento. Define quantas transições são amostradas por iteração de treinamento.\n",
    "                    lr=1e-3,                 # Taxa de aprendizado (learning rate) usada pelo otimizador.\n",
    "                    hidden_size=128,         # Tamanho das camadas ocultas na arquitetura da rede Q.\n",
    "                    gamma=0.99,              # Fator de desconto para o cálculo de recompensa acumulada futura. Valores próximos de 1 dão mais peso a recompensas futuras.\n",
    "                    loss_fn=F.smooth_l1_loss,# Função de perda usada para treinar a rede Q. O padrão é Smooth L1 Loss (Huber Loss).\n",
    "                    optim=AdamW,             # Otimizador usado para atualizar os pesos da rede Q. O padrão é AdamW.\n",
    "                    eps_start=1.0,           # Valor inicial de epsilon na política epsilon-greedy (exploração máxima).\n",
    "                    eps_end=0.15,            # Valor final de epsilon na política epsilon-greedy (exploração mínima).\n",
    "                    eps_last_episode=1000,    # Número de episódios até que o valor de epsilon decresça de `eps_start` para `eps_end`.\n",
    "                    samples_per_epoch=10_000,# Número de amostras a serem coletadas por época para o Replay Buffer.\n",
    "                    sync_rate=10):           # Taxa de sincronização (em épocas) entre a rede Q e a Target Q Network.\n",
    " \n",
    "        super().__init__()\n",
    "        self.env = create_enviroment(env_name)\n",
    "\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.n\n",
    "\n",
    "        self.q_net = DQN(hidden_size, obs_size, n_actions)\n",
    "        self.target_q_net = copy.deepcopy(self.q_net)\n",
    "\n",
    "        self.policy = policy\n",
    "        self.buffer = ReplayBuffer(capacity=capacity)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Fill the experience buffer\n",
    "\n",
    "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
    "            print(f\"{len(self.buffer)} Samples in experience buffer. Filling...\")\n",
    "            self.play_episode(epsilon=self.hparams.eps_start)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_episode(self, policy=None, epsilon=0.):\n",
    "        state = self.env.reset()\n",
    "        done = False  # Initialize done properly\n",
    "\n",
    "        while not done:\n",
    "            if policy:\n",
    "                action = policy(state, self.env, self.q_net, epsilon=epsilon)\n",
    "            else:\n",
    "                action = self.env.action_space.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "        \n",
    "            exp = (state, action, reward, done, next_state)\n",
    "            self.buffer.append(exp)\n",
    "            self.state = next_state\n",
    "\n",
    "\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, x):\n",
    "        return self.q_net(x)\n",
    "\n",
    "    # Configure optimizers\n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
    "        return [q_net_optimizer]\n",
    "\n",
    "    # Create dataloader\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDataset(self.buffer,self.hparams.samples_per_epoch)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    # Training step\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        action = actions.unsqueeze(1)  \n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "\n",
    "        state_action_value = self.q_net(states).gather(1,action)\n",
    "\n",
    "        next_action_values = self.target_q_net(next_states.max(dim=1, keepdim=True))\n",
    "\n",
    "        next_action_values[dones] = 0.0\n",
    "\n",
    "\n",
    "        expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
    "\n",
    "        loss = self.hparams.loss_fn(state_action_value, expected_state_action_values)\n",
    "\n",
    "        self.log('episode/Q-Error', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Training epoch end\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        epsilon = max(\n",
    "\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.current_epoch / self.eps_last_episode\n",
    "        )\n",
    "\n",
    "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
    "\n",
    "        self.log('episode/Return' , self.env.return_queue[-1])\n",
    "\n",
    "        if self.current_epoch % self.hparams.sync_rate  == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Caminho relativo para a pasta que você quer remover\n",
    "relative_path = \"./videos\"\n",
    "\n",
    "# Verificar se a pasta existe antes de tentar removê-la\n",
    "if os.path.exists(relative_path):\n",
    "    shutil.rmtree(relative_path)  # Remove a pasta e todo o conteúdo\n",
    "    print(f\"Pasta '{relative_path}' removida com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = DeepQLearning('LunarLander-v2',\n",
    "                      #capacity=10_000, \n",
    "                      #samples_per_epoch=900_000\n",
    "                      )\n",
    "\n",
    "trainer = Trainer(\n",
    "    #gpus = 1,\n",
    "    max_epochs=100_000,\n",
    "    callbacks=[EarlyStopping(monitor='episode/Return', mode='max', patience = 500)]\n",
    ")\n",
    "\n",
    "trainer.fit(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# trainer = Trainer(logger=logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
