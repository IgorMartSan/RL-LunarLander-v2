{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install gymnasium[box2d]\n",
    "pip install stable-baselines3[extra]: The deep reinforcement learning library.\n",
    "pip install huggingface_sb3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando env.step(action), realizamos esta ação no ambiente e obtemos\n",
    "\n",
    "- observation: O novo estado (st+1)\n",
    "- reward:A recompensa que obtemos após executar a ação\n",
    "- terminated: Indica se o episódio foi encerrado (o agente atingiu o estado terminal)\n",
    "- truncated:Introduzido com esta nova versão, ele indica um limite de tempo ou se um agente sai dos limites do ambiente, por exemplo.\n",
    "- info: Um dicionário que fornece informações adicionais (depende do ambiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "# from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "# from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "from tensorboard import notebook\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnNoModelImprovement, StopTrainingOnRewardThreshold, EvalCallback, CheckpointCallback\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = \"./logs/\"\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Coordenada de almofada horizontal (x)\n",
    "- Coordenada de almofada vertical (y)\n",
    "- Velocidade horizontal (x)\n",
    "- Velocidade vertical (y)\n",
    "- Ângulo\n",
    "- Velocidade angular\n",
    "- Se o ponto de contato da perna esquerda tocou o solo (booleano)\n",
    "- Se o ponto de contato da perna direita tocou o solo (booleano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ação 0: Não fazer nada,\n",
    "- Ação 1: Disparar motor de orientação à esquerda,\n",
    "- Ação 2: Disparar o motor principal,\n",
    "- Ação 3: Disparar motor de orientação correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambiente Vetorizado\n",
    "- Criamos um ambiente vetorizado (um método para empilhar vários ambientes independentes em um único ambiente) de 16 ambientes. Dessa forma, teremos experiências mais diversas durante o treinament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "#env = make_vec_env('LunarLander-v3', n_envs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "\n",
    "# Callback para salvar checkpoints durante o treinamento\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=100,  # Frequência de salvamento em passos de treinamento. Aqui, o modelo será salvo a cada 100 passos.\n",
    "    save_path=\"./logs/\",  # Caminho onde os checkpoints serão salvos.\n",
    "    name_prefix=\"rl_model\",  # Prefixo usado nos nomes dos arquivos salvos. Exemplo: rl_model_step_x.zip.\n",
    "    save_replay_buffer=True,  # Salvar o buffer de replay (usado em algoritmos off-policy como DQN ou SAC).\n",
    "    save_vecnormalize=True,  # Salvar o estado da normalização de vetores (caso seja usada com `VecNormalize`).\n",
    ")\n",
    "\n",
    "# Callback para avaliação do desempenho do modelo durante o treinamento\n",
    "eval_callback = EvalCallback(\n",
    "    env,  # Ambiente usado para avaliação.\n",
    "    eval_freq=10000,  # Frequência de avaliação, em passos de treinamento. Aqui, o modelo será avaliado a cada 10.000 passos.\n",
    "    callback_on_new_best=callback_on_best,  # Callback opcional a ser chamado quando o modelo atingir uma nova melhor avaliação.\n",
    "    callback_after_eval=stop_train_callback,  # Callback opcional a ser executado após cada avaliação, como parar o treinamento se uma condição for atingida.\n",
    "    verbose=1,  # Nível de verbosidade. `1` exibe informações sobre o progresso da avaliação.\n",
    "    best_model_save_path=os.path.join(model_dir, f\"{args.gymenv}_{args.sb3_algo}\"),  \n",
    "    # Caminho para salvar o melhor modelo encontrado durante as avaliações. \n",
    "    # Aqui, combina o diretório base (model_dir) com o nome do ambiente (gymenv) e do algoritmo (sb3_algo).\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = gym.make('LunarLander-v3',  render_mode=\"rgb_array\")\n",
    "# Save a checkpoint every 1000 steps\n",
    "env = gym.wrappers.RecordVideo(env=env, video_folder=\"./video_trainig/\", name_prefix=\"test-video\", episode_trigger=lambda x: x % 100 == 0)\n",
    "# Instantiate the agent\n",
    "env = Monitor(env)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, n_steps=200, tensorboard_log=log_dir)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(100_000), callback=model.save(\"ppo_cartpole\"),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "# env = gym.make('LunarLander-v3')\n",
    "# obs = env.reset()\n",
    "# print(obs[0])\n",
    "# for _ in range(20):\n",
    "#     action, _states = model2.predict(obs[0])\n",
    "#     print(action)\n",
    "#     observation , rewards, terminated ,truncated , info = env.step(action)\n",
    "#     #env.render(\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos Gráficos de Treinamento\n",
    "\n",
    "### 1. `rollout/ep_len_mean` (Comprimento Médio do Episódio)\n",
    "#### **Descrição**\n",
    "- Este gráfico mostra o comprimento médio dos episódios ao longo do treinamento.\n",
    "- Um comprimento maior geralmente indica que o agente está melhorando seu desempenho no ambiente.\n",
    "\n",
    "#### **Indicadores**\n",
    "- **Bons:**\n",
    "  - Aumento constante no comprimento médio ao longo do tempo.\n",
    "  - Indica que o agente está sobrevivendo mais tempo ou completando tarefas de forma eficiente.\n",
    "- **Ruins:**\n",
    "  - Estagnação ou queda no comprimento médio.\n",
    "  - Pode indicar que o agente está tendo dificuldades para aprender.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `rollout/ep_rew_mean` (Recompensa Média por Episódio)\n",
    "#### **Descrição**\n",
    "- Este gráfico mede a recompensa média obtida pelo agente em cada episódio.\n",
    "- Recompensas mais altas indicam que o agente está aprendendo a maximizar o objetivo definido.\n",
    "\n",
    "#### **Indicadores**\n",
    "- **Bons:**\n",
    "  - Recompensas crescentes ao longo do treinamento.\n",
    "  - Indica que o agente está melhorando suas decisões no ambiente.\n",
    "- **Ruins:**\n",
    "  - Recompensas negativas ou que não apresentam crescimento.\n",
    "  - Sinal de que o agente está estagnado ou não está aprendendo de forma eficaz.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `time/fps` (Frames por Segundo)\n",
    "#### **Descrição**\n",
    "- Este gráfico exibe a eficiência computacional do treinamento, medindo o número de frames por segundo (FPS) processados.\n",
    "\n",
    "#### **Indicadores**\n",
    "- **Bons:**\n",
    "  - FPS elevado e estável (acima de 300).\n",
    "  - Indica que o treinamento está sendo executado de forma eficiente.\n",
    "- **Ruins:**\n",
    "  - Quedas abruptas ou FPS consistentemente abaixo de 200.\n",
    "  - Pode ser um indicativo de sobrecarga no hardware ou aumento da complexidade computacional.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusão Geral\n",
    "1. **`rollout/ep_len_mean`**: O comprimento médio dos episódios está aumentando, o que é um sinal positivo de aprendizado.\n",
    "2. **`rollout/ep_rew_mean`**: A recompensa média também está crescendo, indicando que o agente está melhorando seu desempenho.\n",
    "3. **`time/fps`**: Apesar de uma queda gradual no FPS, ele permanece em um nível aceitável (~285 FPS), mantendo o treinamento eficiente.\n",
    "\n",
    "Esses gráficos mostram que o treinamento está funcionando bem, tanto em termos de aprendizado do agente quanto de eficiência computacional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
